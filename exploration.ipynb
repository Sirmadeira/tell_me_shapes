{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af8f77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (12.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.3.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: torch==2.9.1 in ./.venv/lib/python3.12/site-packages (from torchvision) (2.9.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pillow\n",
    "%pip install numpy\n",
    "%pip install torch\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf9a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ee3ed",
   "metadata": {},
   "source": [
    "# First step - Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d600e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageDraw;\n",
    "\n",
    "# The core structure\n",
    "img = Image.new(\"RGB\",(128,128),color= \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756ce0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that can be printed out\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "\n",
    "draw.circle((64,64),radius=30,fill=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58bdf4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACo554bW3luLiWOGCJC8kkjBVRQMkkngADnNE88Nrby3FxLHDBEheSSRgqooGSSTwABzmvlj4s/FmbxlcPo+jvJD4fifk4KteMDwzDqEB5VT/ALx5wFAPR/F37Qei6S93Y6BaSaneROY1uGYLak7T8ykEs4DYGAFDDJDYwT5hqXx58eX1wstvfWmnoECmK1tEZScn5j5m855x1xwOOufM6KAPSLH46+PrS8jnm1SC9jXOYJ7SMI+QRyUVW468EdPTivQ/C/7RtjOiQeKNMktZy6r9psRviwWOWZGO5Qo29C5PPA4FfOlFAH3vY39nqdnHeWF3Bd2smdk0EgkRsEg4YcHBBH4VYr48+GfxMvvAGqFHElzotw4N1aA8g9PMjzwHA7dGAwegK/W+larY65pdvqemXMdzZ3Cb4pU6MP5gg5BB5BBBwRQBcooooAKKKKACiiigAooooA8b/aD8XNpPhq10CxvZIbzUXLXCxMuTbAEFW53AMxAGBhgjgnGQfmSvTPjzqU198VLy3lWMJYW8NvEVByVKCXLc9d0jDjHAH1PmdABRRRQAUUUUAFe+fs6+LmFxfeFb29kKMguNPikZdqkE+aq5OcnIbaMj5XPHOfA66z4Y6lNpXxN8O3ECxs73qW5DgkbZT5THgjna5I98delAH2nRRRQAUUUUAFFFFABRRRQB8kfHWxuLT4r6lNPHsju4oJoDuB3oI1jJ46fMjDn09MV5vX0X+0b4XWfS9P8AFEEchntnFnc7UZh5TZZGY5woVsjpyZRzwBXzpQAUUUUAFFFFABXUfDixuNQ+JXhyG1j8yRdQhmI3AYSNhI559FVj7445rl69s/Z18LrfeIL7xJcRybNOQQ2pKMFMsgIYhs4JVOCuD/rQeMDIB9J0UUUAFFFFABRRRQAUUUUAV7+xt9T065sLyPzLW6ieGZNxG5GBDDI5GQT0r5E+JnwzvvAGqB0Mlzotw5Frdkcg9fLkxwHA79GAyOhC/YdU9V0qx1zS7jTNTto7mzuE2SxP0YfzBBwQRyCARgigD4Mor3zxd+zrMHu73wrqEbIXLxabcgqVXaTtWXJ3HdgDcBweW4yfL9S+GPjfSrhYLjwxqTuyBwbWE3C4yRy0e4A8dM56eooA5Oiuosfhx411C8jtYfC+qpI+cGe2aFBgE8u4Cjp3PPTrXonhf9nXV75EuPEmoR6Ym9SbWACaUqGO4FgdiEgAgjf97kDGKAPO/AvgXVPHmuCwsB5VvHhrq7dcpboe59WODhe+OwBI+w/DmgWPhbw/Z6LpqyC0tUKp5jbmYklmYn1LEnjA54AHFGgeHNI8LaWum6LYx2loHL7FJYsx6lmYksegySeAB0ArUoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADSElEQVR4Ae2c21LbUAwA607//5eDGRfjoS5yDkdaJVkewGR0dNm1HB6A5Xa7/fKDI/CbK23ldwIKgO8DBSgAJgCXdwMUABOAy7sBCoAJwOXdAAXABODyboACYAJweTdAATABuLwboACYAFzeDVAATAAu7wYoACYAl3cDFAATgMu7AQqACcDl3QAFwATg8m6AAmACcHk3QAEwAbi8G6AAmABc3g2ABfyB699fflmW7w891t/8LA/Rbgj9f0r6T9ddwDD6o5LOGpoKmML96GC7bmii45twEv3VQV7mf2VffKXXBpQB6rMKjTagjH6rVegioJL+9nCor3j6UGohgGJB1T2a4AWwFNjqqwlYAD7/ioDtgRTATn58DoCdYALAmY/o92uqH0YANe2O+/QC6YoRcDr/a74ICEButIt263urFlA/4UX0e1hxh9UC9jm92AgoAL4TSgUUb/cw2so+SwUME3nig3UCKm+rnwsr67ZOwM+hPGUGBcBaFfAaAsoeqRNx1vTsBkxUNpJKASPUJp5RwESYI6kUMEJt4hkFTIQ5kkoBI9QmnlHARJgjqRQwQm3iGQVMhDmSSgEj1CaeKRLQ59fBr7Or6blIwPWxXy1SAbBxBbyMgJpH6iycZd26AbOUDeYpFVB2Ww3C+DhW2WepgI8B/fpJQAGfLJCragGV2z0GtLjDagErlOIJ79JQ3xsg4C4iTx/MCKi/0a6IRLpiBKw4kGm/0UD1gwlo5YCiv0IgBTRxANLnBeAOWPotBIAOcPpdBCAOOtBvJKDYQRP669S9/mXZ2tB7T9F/Bt3Cxj73Qb/1D/8UdAoxj1Fe5tNBrrzYcQOOfU/Zhobc9xm7C9gaHdbQGf3f0fq3uN8sF2U81kSPsQFfHDzTtx3fhJ+JbziLAkJEuQEKyOUbZldAiCg3QAG5fMPsCggR5QYoIJdvmF0BIaLcAAXk8g2zKyBElBuggFy+YXYFhIhyAxSQyzfMroAQUW6AAnL5htkVECLKDVBALt8wuwJCRLkBCsjlG2ZXQIgoN0ABuXzD7AoIEeUGKCCXb5hdASGi3AAF5PINsysgRJQboIBcvmF2BYSIcgMUkMs3zK6AEFFugAJy+YbZFRAiyg14A8yHjdtgDOdeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad189e4",
   "metadata": {},
   "source": [
    "# Second step - Make a bunch of random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6933b6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADK0lEQVR4Ae2c4U7DMAwGKeL9X3l0KqrMNm2QJj43O34ZWPI5d/EAaWy5XC4ffnAEPrlok68EFADfAwUoACYAxzsBCoAJwPFOgAJgAnC8E6AAmAAc7wQoACYAxzsBCoAJwPFOgAJgAnC8E6AAmAAc7wQoACYAxzsBCoAJwPFOgAJgAnC8E6AAmAAc7wQoACYAxzsBCoAJwPFOgAJgAnC8E6AAmAAc7wQoACYAxzsBCoAJwPFOgAJgAnC8EwAL+ILzD8cvy/J8j+JvBbAU7+8h3JfQH65av1jwsCcT0Iw+Kiml4TQCuqAvqOEEArqjL6Wh+m9BQ+mvJkbvH2U/rEsLyKGTk/KQ/vUGlPqJtHeJQEFQVJwAhP71Mr76k2K/Hx2LcgIQCjvQ/PRyAnYWb1LUEpB/Ae81J/dQSEDyye/R71/J7KSKgMwz76CfFGn9VBHwhMXc3yohIO26/ctlTlclBPyLy2QP5gXkXLQ2bQm98QLa0EyzChaQcMUOqhrdISzgIJ0JlisAlqiANxYw+um1F9qhfToBvTQ17qOARnC9limgF8nGfRTQCK7XMgX0Itm4jwIawfVapoBeJBv3UUAjuF7LFNCLZOM+CmgE12sZKQB5KWADuKF9kgIaWMy3RAGwUwW8t4ChT69d0I7u0Anooql9E17A6CvWzibl31p5AUcATbC2hICaQ5DTVQkBE1zk5iNUEZBz3f6OKa2fKgJWNGlnfqkhs5NCAoo4yKS/HrmWgJd3c74HlBOQfAFvjOanlxOwEsmnsGlAcou+VcFGZOhrAuPdR9BvDVScgB1NDpeclP1QN0VpAWuvo+mM3v8G9/2npZ+CYrvdn45w9NvpTiPgp90eb2hSBP0pBRzUUAr9iQVsrf9RRkHov/ov3l/sdcq6+m9BU0KPh1JApAHUCgCgx0gFRBpArQAAeoxUQKQB1AoAoMdIBUQaQK0AAHqMVECkAdQKAKDHSAVEGkCtAAB6jFRApAHUCgCgx0gFRBpArQAAeoxUQKQB1AoAoMdIBUQaQK0AAHqMVECkAdQKAKDHSAVEGkCtAAB6jFRApAHUCgCgx0gFRBpA/Q3vfYfTMLKl2gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADYElEQVR4Ae2c0ZKbMAxFS6f//8spO+lQRglGGFvXMmef8Czxlc+RSB4yWV6v1y/+dAR+66JJ/iGAAHEfIAABYgLieCYAAWIC4ngmAAFiAuJ4JgABYgLieCYAAWIC4ngmAAFiAuJ4JgABYgLieCYAAWIC4ngmAAFiAuJ4JgABYgLieCYAAWIC4ngmAAFiAuJ4JgABYgLieCYAAWIC4ngmAAFiAuJ4JgABYgLieCYAAWIC4ngmAAFiAuJ4JgABYgLieCZALOCPOH+A+GVZylV0/TGBpevu5YMJ/3sK/ai25rieJaCa+6ePViaeIqAh+r2M+xrmF9AJfSsNMwsIQH9fw7QfQ4PprybqEucUUMdi38511xW5sz2CKhDUsS6/yv/mPNUEDEJ/deOvZB4B/jOXm7fVf531TCLAedpWcJ37eKqaRICTyIC3zSDA02gq9Ke1pRdwekIV+i23XGFuAeWzbQjkF4U6cwuQk71fQGIBhba6z6X5DkfVJhbQnJFkw6wCjhpKAtEZ+rXmrAKcZx7/tpQCvrbS+KzXCj8rTykgBWtnkQhwgup1Wz4Bn1Pci02ffU39+QT0wSLbFQEy9O9gBCDgCgHzAL3y0oHu3Z+CCRCLQQACxATE8UwAAsQExPFMAALEBMTxTAACxATE8UwAAq4Q8H/t+8qu0ffuT8EERNM3eQgwQKKXCIgmbvLyCdg/QM1hUixN/fkEpKDsLxIBflZd7kwpwExxFzB9Nv2sPKWAPnA0u2YV8NlKGn5XUr/WnFXAlYMPfW9iAV8baljYR9UmFjAs60uF5RZw1FaXEATcXKgzt4CVXeFsAWQ9EeUK0wsY3EGZ/lr8DAI8bTjsPZMIOG00iQBPVZMIWPl6ThupwVnPPAKGcuCkv9Y820+WvXt8//3vyK6vaIKpJmBj7W/A7SVNLipy5xRQ0Yn3BVTQX0PnfATtaQY8jurQv4ucX8C/c579Rv3emf/6DvpnCWiu4T76JwrYWrv6udSK+/9Kmu+4bZ3l4lRGV0RPeQ8Ythum/Rg6LHFTGAIMkOglAqKJmzwEGCDRSwREEzd5CDBAopcIiCZu8hBggEQvERBN3OQhwACJXiIgmrjJQ4ABEr1EQDRxk4cAAyR6iYBo4iYPAQZI9BIB0cRNHgIMkOjlX9ehn8tfp7WWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "l = []\n",
    "for i in range(5000):\n",
    "    img = Image.new(\"RGB\",(128,128),\"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Stay in side\n",
    "    radius = random.randint(10,40)\n",
    "    # For now we ensure the shape is inside the canva\n",
    "    center = (random.randint(radius,128-radius),random.randint(radius,128-radius))\n",
    "    draw.circle(xy=center,radius=radius,fill=\"black\")\n",
    "    l.append(img)\n",
    "    img.save(f\"data/train/circles/{i}.png\")\n",
    "\n",
    "[display(img) for img in l[50:52]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbde7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(5000):\n",
    "    img = Image.new(\"RGB\",(128,128),\"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    points = [\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "]\n",
    "    draw.polygon(points,fill=\"black\")\n",
    "    l.append(img)\n",
    "    img.save(f\"data/train/triangles/{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfa279",
   "metadata": {},
   "source": [
    "# Third make the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2384bb",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c92535bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a31fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapeCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=16384, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A nice way to get the accelerato (whonever is training)\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class ShapeCNN(nn.Module):\n",
    "    # Define the layers of the neural net\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride = 1,padding=1)\n",
    "\n",
    "\n",
    "        # Diminish our image size by 2, note that does make it so it need to be divisible by 2 our input\n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # Diminishes our tensor size and spitss out new weights to balance it out considering the previous weights\n",
    "        self.fc1 = nn.Linear(in_features=(64*16*16),out_features=256)\n",
    "        self.fc2 = nn.Linear(256,2)\n",
    "\n",
    "\n",
    "\n",
    "    # How tensors are treated as we move along the net neurons\n",
    "    def forward(self,x):\n",
    "        # Here we are in summary ignoring negative values and making positive values variant\n",
    "        # A good metaphor is thinking that we are only getting the good tastes and not the bad ones\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "\n",
    "        # This immediatelly creates the layer and inputs the x given by our neurons layers\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x \n",
    "\n",
    "    \n",
    "\n",
    "# A nice way to see my model layers\n",
    "model = ShapeCNN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef429021",
   "metadata": {},
   "source": [
    "# Step 4 - Get my training data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a152b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ecdcd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data/train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.ImageFolder(\"data/train\",transform=transform)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8ae438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf10d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 128, 128])\n",
      "Labels batch shape: torch.Size([64])\n",
      "[1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# Notice how we have inherited labels according to our previous division\n",
    "print(train_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60acc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
