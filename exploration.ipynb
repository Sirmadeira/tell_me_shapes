{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af8f77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (12.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.3.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: torch==2.9.1 in ./.venv/lib/python3.12/site-packages (from torchvision) (2.9.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pillow\n",
    "%pip install numpy\n",
    "%pip install torch\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf9a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ee3ed",
   "metadata": {},
   "source": [
    "# First step - Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d600e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageDraw;\n",
    "\n",
    "# The core structure\n",
    "img = Image.new(\"RGB\",(128,128),color= \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756ce0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that can be printed out\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "\n",
    "draw.circle((64,64),radius=30,fill=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58bdf4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADSElEQVR4Ae2c21LbUAwA607//5eDGRfjoS5yDkdaJVkewGR0dNm1HB6A5Xa7/fKDI/CbK23ldwIKgO8DBSgAJgCXdwMUABOAy7sBCoAJwOXdAAXABODyboACYAJweTdAATABuLwboACYAFzeDVAATAAu7wYoACYAl3cDFAATgMu7AQqACcDl3QAFwATg8m6AAmACcHk3QAEwAbi8G6AAmABc3g2ABfyB699fflmW7w891t/8LA/Rbgj9f0r6T9ddwDD6o5LOGpoKmML96GC7bmii45twEv3VQV7mf2VffKXXBpQB6rMKjTagjH6rVegioJL+9nCor3j6UGohgGJB1T2a4AWwFNjqqwlYAD7/ioDtgRTATn58DoCdYALAmY/o92uqH0YANe2O+/QC6YoRcDr/a74ICEButIt263urFlA/4UX0e1hxh9UC9jm92AgoAL4TSgUUb/cw2so+SwUME3nig3UCKm+rnwsr67ZOwM+hPGUGBcBaFfAaAsoeqRNx1vTsBkxUNpJKASPUJp5RwESYI6kUMEJt4hkFTIQ5kkoBI9QmnlHARJgjqRQwQm3iGQVMhDmSSgEj1CaeKRLQ59fBr7Or6blIwPWxXy1SAbBxBbyMgJpH6iycZd26AbOUDeYpFVB2Ww3C+DhW2WepgI8B/fpJQAGfLJCragGV2z0GtLjDagErlOIJ79JQ3xsg4C4iTx/MCKi/0a6IRLpiBKw4kGm/0UD1gwlo5YCiv0IgBTRxANLnBeAOWPotBIAOcPpdBCAOOtBvJKDYQRP669S9/mXZ2tB7T9F/Bt3Cxj73Qb/1D/8UdAoxj1Fe5tNBrrzYcQOOfU/Zhobc9xm7C9gaHdbQGf3f0fq3uN8sF2U81kSPsQFfHDzTtx3fhJ+JbziLAkJEuQEKyOUbZldAiCg3QAG5fMPsCggR5QYoIJdvmF0BIaLcAAXk8g2zKyBElBuggFy+YXYFhIhyAxSQyzfMroAQUW6AAnL5htkVECLKDVBALt8wuwJCRLkBCsjlG2ZXQIgoN0ABuXzD7AoIEeUGKCCXb5hdASGi3AAF5PINsysgRJQboIBcvmF2BYSIcgMUkMs3zK6AEFFugAJy+YbZFRAiyg14A8yHjdtgDOdeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad189e4",
   "metadata": {},
   "source": [
    "# Second step - Make a bunch of random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6933b6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiq99f2emWcl5f3cFpax43zTyCNFyQBljwMkgfjQBYorxvX/wBorw5p7tFoun3erOrgeYx+zxMpXJKlgXyDgYKDvz0zl2P7S9nJeRrf+GJ4LU53yQXgldeDjClFB5x/EPXnpQB7xRXN+FPHnhzxnbh9G1GOScJuktJPknj4XOUPJALAbhlc8AmukoAKKKKACiiigAooooAKKKKACiiigCvf31vpmnXN/eSeXa2sTzTPtJ2ooJY4HJwAelfHHxA+IGqeONcuZZbqddJWUmzsidqRoMhGZASDJgnLZPUgHGAPa/2itfbT/B1josTSK+qXBaTCqVaKLDFSTyDvaIjH9089j8yUAFFFFAFixv7zTLyO8sLue0uo87JoJDG65BBww5GQSPxr63+Enj//AITrwv8A6ZJu1mw2x3uItitktscY4+YKc4xhg3AGM/IFd58HNfbw/wDE3Sm3SeRfP9hmVFViwkICDnoBJ5ZJHOAevQgH2HRRRQAUUUUAFFFFABRRRQAUUUUAeD/tL2NxJp3h2/WPNrDLPDI+4fK7hCox15Eb/l7ivnivtvx54Uh8Z+Dr/RnEYndN9rI+P3cy8oc4JAz8pIGdrMB1r4sv7G40zUbmwvI/LurWV4Zk3A7XUkMMjg4IPSgCvRRRQAV1Hw4sbjUPiV4chtY/MkXUIZiNwGEjYSOefRVY++OOa5evoP8AZ68DNEkvjK/ijIlRoNOyVYgbiskmMZU5XYDkHG/IwQSAe+UUUUAFFFFABRRRQAUUUUAFFFFABXn/AI/+EmheOt95/wAg/WW2D7fEhbcq8YdMgNwcZ4YYXnAwfQKKAPjzX/g5438Pu27R5NQg3hFm07M4Ylc/cA3gDkElQMj3Gcux+HHjXULyO1h8L6qkj5wZ7ZoUGATy7gKOnc89Otfa9FAHgfgb9npYnjv/ABlNHMChP9mW7sACVGN8ikHIJb5V4yAdxGQfeIIIbW3it7eKOGCJAkccahVRQMAADgADjFSUUAFFFFABRRRQAUUUUAFFFFAH/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAACrUlEQVR4Ae2d0WoCQRAEcyH//8tGubARREG3Z0pvy6dDwvRc1fb5IJLtdDp9+eIIfHPRJl8IKAA+BwpQAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAFjAT3P+tm33Etf8r5Zbz20/4H7ro2el21zknXIBT6G/RrCIhkIBL6NfSkPVh3CE/tlEas611Le6zjegCNlRn0jhBhTRP3AVkgLq6O8Pjer5yKMpKQC5gU8PjQnoOZ49KZ1SMwI6uXRmNZjICGhY9KgRAQH9R7I/sU5/QEDdcitMnhVAHUYqN34mZgXEF1ptoAJg41MC2OcAm57yNiUgtcTKcxQA21eAAmACcLwNUABMAI63AQqACcDxUw1gvyhn01PepgSkllh5jgJg+7MCqOcAlRvXNSsgvtBqAwMC+g9jf2LdsQgIqFtuhckZAZ1HsjOr4QRkBJwX7eHSk9LAfUTEBIyJXjxFICmg+nhWz38KXOqPkwLOO9UxqpucQvnanPwPNPY9gt+YHxX9DircgHEKUtRSc8Zi73ZR1YBxny9X4fDod0TlAv5i7v8+e6gaF4ugbxUw4D4oxFLc/4Gsedvj/vGLqg9h/MY+ZQEFwKYUoACYABxvAxQAE4DjbYACYAJw/C8b7Wnb/MKHDQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAC5UlEQVR4Ae2d4U7jMBAGL6d7/1cuQT2hQmnYtb2dshl+oJBY/uwZf1WRUNkul8sfvzgCf7lok98JKAA+BwpQAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgOP/wfmvEb9t26OFVP+Di6064NHG8PsH0B+trYLVGQUMoL9VslbDuQRMoq/QcBYBC9Gv1XCKt6FF9HcT8zP3FzDP6PbI319Pzt9cwCSde9zf3plJ6Sxghsu3oA9uDme1FTBM5IDy8aOxxJ4Cxlgc8408HchtKGCAQgRucEw2vaGAIKkXGdZNQPYAVmhIraGbgAqgpXO2EpA6eqVY4ytpJaCUadHkfQTED10Ryi/TBtfTR8CX/f+WHxUAm1KAAlYQCL7grohKzBFZlQ1IAK0YqoAKqok5FZCAVTFUARVUE3MqIAGrYqgCKqgm5lRAAlbFUAVUUE3M2UTA2r/XTPA7HBpZVRMBhxxe+qECYD0KUMAiApEX3EVRoWmC67EBIZp1g1oJCB66OpofM8dX0krAx/5/0UU3AfGjVycptYZuAuqwFs3cUEDqAC7Hmk1vKGBnmqWwSsNAbk8BiIMB+vs62wp4soMx+s0FPM3BMP3+Ap7gYIb+KQSUOpikv6/tLB9VsG/1fbePP5bmOiD+fR79NetcAv7veU7DKvTnFTCsYS36swu47v9HGRXQP0VXB9yGeX1PoPMvYve7fcE7CoClKEABMAE43gYoACYAx9sABcAE4HgboACYABxvA2ABbzvFe9vOmiRBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "l = []\n",
    "for i in range(5000):\n",
    "    img = Image.new(\"RGB\",(128,128),\"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Stay in side\n",
    "    radius = random.randint(10,40)\n",
    "    # For now we ensure the shape is inside the canva\n",
    "    center = (random.randint(radius,128-radius),random.randint(radius,128-radius))\n",
    "    draw.circle(xy=center,radius=radius,fill=\"black\")\n",
    "    l.append(img)\n",
    "    img.save(f\"data/train/circles/{i}.png\")\n",
    "\n",
    "[display(img) for img in l[50:52]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbde7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(5000):\n",
    "    img = Image.new(\"RGB\",(128,128),\"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    points = [\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "]\n",
    "    draw.polygon(points,fill=\"black\")\n",
    "    l.append(img)\n",
    "    img.save(f\"data/train/triangles/{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfa279",
   "metadata": {},
   "source": [
    "# Third make the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2384bb",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c92535bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a31fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapeCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=16384, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A nice way to get the accelerato (whonever is training)\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class ShapeCNN(nn.Module):\n",
    "    # Define the layers of the neural net\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride = 1,padding=1)\n",
    "\n",
    "\n",
    "        # Diminish our image size by 2, note that does make it so it need to be divisible by 2 our input\n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        #   Convert our bias and weight that we got from the neurons into a 256 a number i got from my ass.\n",
    "        # So we dont have overfitting, this applies an afiine linear transformation which is in summary\n",
    "        # A fancy way transformation of our previous vector into another vector, which follows a series of guidelines\n",
    "        self.fc1 = nn.Linear(in_features=(64*16*16),out_features=256)\n",
    "        self.fc2 = nn.Linear(256,2)\n",
    "\n",
    "\n",
    "\n",
    "    # How tensors are treated as we move along the net neurons\n",
    "    def forward(self,x):\n",
    "        # Here we are in summary ignoring negative values and making positive values variant\n",
    "        # A good metaphor is thinking that we are only getting the good tastes and not the bad ones\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "\n",
    "        # This immediatelly creates the layer and inputs the x given by our neurons layers\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x \n",
    "\n",
    "    \n",
    "\n",
    "# A nice way to see my model layers\n",
    "model = ShapeCNN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef429021",
   "metadata": {},
   "source": [
    "# Step 4 - Get my training data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a152b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ecdcd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data/train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.ImageFolder(\"data/train\",transform=transform)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8ae438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebf10d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 128, 128])\n",
      "Labels batch shape: torch.Size([64])\n",
      "[1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# Notice how we have inherited labels according to our previous folder division\n",
    "print(train_labels.tolist()[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_loop(model, train_loader,loss_fn,optimizer):\n",
    "    # You need to tell the model he is in training mode\n",
    "    model.train()\n",
    "    for images,labels in train_loader:\n",
    "        # Mode the data into the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Attempt at predict - Learning part\n",
    "        pred = model(images)\n",
    "        loss = loss_fn(pred,labels)\n",
    "        # Backpropagate - THE CHAIN RULE\n",
    "        loss.backward()\n",
    "        # Applies the gradients found by backpropagation\n",
    "        optimizer.step()\n",
    "        # Reset to not resum\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "903d0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "train_loop(model=model,train_loader=train_loader,loss_fn = loss_fn,optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd641b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
