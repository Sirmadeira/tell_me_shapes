{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4af8f77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (12.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.3.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: torch==2.9.1 in ./.venv/lib/python3.12/site-packages (from torchvision) (2.9.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pillow\n",
    "%pip install numpy\n",
    "%pip install torch\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adf9a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ee3ed",
   "metadata": {},
   "source": [
    "# First step - Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d600e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageDraw;\n",
    "\n",
    "# The core structure\n",
    "img = Image.new(\"RGB\",(128,128),color= \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "756ce0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that can be printed out\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "\n",
    "draw.circle((64,64),radius=30,fill=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58bdf4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADSElEQVR4Ae2c21LbUAwA607//5eDGRfjoS5yDkdaJVkewGR0dNm1HB6A5Xa7/fKDI/CbK23ldwIKgO8DBSgAJgCXdwMUABOAy7sBCoAJwOXdAAXABODyboACYAJweTdAATABuLwboACYAFzeDVAATAAu7wYoACYAl3cDFAATgMu7AQqACcDl3QAFwATg8m6AAmACcHk3QAEwAbi8G6AAmABc3g2ABfyB699fflmW7w891t/8LA/Rbgj9f0r6T9ddwDD6o5LOGpoKmML96GC7bmii45twEv3VQV7mf2VffKXXBpQB6rMKjTagjH6rVegioJL+9nCor3j6UGohgGJB1T2a4AWwFNjqqwlYAD7/ioDtgRTATn58DoCdYALAmY/o92uqH0YANe2O+/QC6YoRcDr/a74ICEButIt263urFlA/4UX0e1hxh9UC9jm92AgoAL4TSgUUb/cw2so+SwUME3nig3UCKm+rnwsr67ZOwM+hPGUGBcBaFfAaAsoeqRNx1vTsBkxUNpJKASPUJp5RwESYI6kUMEJt4hkFTIQ5kkoBI9QmnlHARJgjqRQwQm3iGQVMhDmSSgEj1CaeKRLQ59fBr7Or6blIwPWxXy1SAbBxBbyMgJpH6iycZd26AbOUDeYpFVB2Ww3C+DhW2WepgI8B/fpJQAGfLJCragGV2z0GtLjDagErlOIJ79JQ3xsg4C4iTx/MCKi/0a6IRLpiBKw4kGm/0UD1gwlo5YCiv0IgBTRxANLnBeAOWPotBIAOcPpdBCAOOtBvJKDYQRP669S9/mXZ2tB7T9F/Bt3Cxj73Qb/1D/8UdAoxj1Fe5tNBrrzYcQOOfU/Zhobc9xm7C9gaHdbQGf3f0fq3uN8sF2U81kSPsQFfHDzTtx3fhJ+JbziLAkJEuQEKyOUbZldAiCg3QAG5fMPsCggR5QYoIJdvmF0BIaLcAAXk8g2zKyBElBuggFy+YXYFhIhyAxSQyzfMroAQUW6AAnL5htkVECLKDVBALt8wuwJCRLkBCsjlG2ZXQIgoN0ABuXzD7AoIEeUGKCCXb5hdASGi3AAF5PINsysgRJQboIBcvmF2BYSIcgMUkMs3zK6AEFFugAJy+YbZFRAiyg14A8yHjdtgDOdeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad189e4",
   "metadata": {},
   "source": [
    "# Second step - Make a bunch of random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6933b6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADFklEQVR4Ae2c0U7DQAwECeL/f7mkCqqCilPU2Lt2NDygkkO3vplsygPqcrvdPvjyEfj0RZN8J4AA832AAASYCZjjaQACzATM8TQAAWYC5ngagAAzAXM8DUCAmYA5ngYgwEzAHE8DEGAmYI6nAQgwEzDH0wAEmAmY42kAAswEzPE0AAFmAuZ4GoAAMwFzPA1AgJmAOZ4GIMBMwBxPAxBgJmCOpwEIMBMwx9MABJgJmONpAALMBMzxNAABZgLmeBqAADMBczwNQICZgDmeBiDATMAcTwMQYCZgjqcBZgFf5vyy+GVZjvdu8lFhS5M5jmH9c/Ul9GgfI4SLCHgb/V6JRcNsASnc9w6210oTg9+Ei+ivDup2fpY9sgEyQIIqzGuAjL6mCsMEKOlvj4vqxEkCqlk8P6AFDsYIcNGvdjBDgJd+qYMBAjrQr3PQXUAf+kUOWgvoRr/CQWsB24Gv/b2vgJ63f3oJmgroTD/XQVMB137s7E+HgD0Nw+uOAvo/fzZRKXN2FGC4D32R7QSk3FYynuenbSdAxq5JEALMIhCAgB2B84/U3WailydnpgEiT1EMAiIyousIEIGOYhAQkRFdR4AIdBSDgIiM6DoCRKCjGAREZETXESACHcUgICIjut5LgODfwdO5npy5l4B0Ov03RIDZEQIQ8JvAyUfq783Kfzo/LQ0ol3Qc0FHA+dvq+MxZqylzdhSQBWjEPggwa2oqIKXdpWizJmwqYGWXdcIKDYmz9RVQAa7hnq0FJN5oiehzp2otYKWWe9rzGtLn6S6glYN0+uvpBgho4qCC/hgBdgdF9CcJMDqooz9MgMVBKf15AsQOqumvxxn5kWXr3PfRX30y6PZr730XoN8Gm/FX0J8Q6xjV7fx8kMEN2B8mpQ1K7o/hLyJgO8/bGizof2Y2Zj/ugooXL2U0OfilGlAhsnrPwW/C1Wg0+yNAwzlMQUCIRrOAAA3nMAUBIRrNAgI0nMMUBIRoNAsI0HAOUxAQotEsIEDDOUxBQIhGs4AADecwBQEhGs0CAjScwxQEhGg0CwjQcA5TvgGyPo3bUZsjKwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADZ0lEQVR4Ae2d0XKjMBAEj6v7/192SFGlI8YYabXSDE7nSSFCu9vNOHmxszwejz986Qj81ZWm8jcBBIifAwQgQExAXJ4EIEBMQFyeBCBATEBcngQgQExAXJ4EIEBMQFyeBCBATEBcngQgQExAXJ4EIEBMQFyeBCBATEBcngQgQExAXJ4EIEBMQFyeBCBATEBcngQgQExAXJ4EIEBMQFyeBCBATEBcngQgQExAXJ4EIEBMQFyeBCBATEBcngQgQExAXJ4EIEBMQFz+n7D+sizvq/+Gj1FYJg95Cf1MyeQ+z9pIvz5JQJj7ceAPMzFWQCL3TzUx8K+gofRXH6PPPyofcWVIAiajufWLUn4CJtO/exQyEzAf/dNrwh2jkJYAOf2bRiFHgAP9LQ0+nTyl8+zbBAFuM7v1c4Z+u94rwHNaz65emugS4Dync297E3EB/hP6d7iaiAvYa2QdJhAUcIuHa4Xi32dEgP9U++fRvNuIgP14rDsJNAswf6Be4nDuuU2A8yQv0ZeLtp23CSjzsMgigIAsksFzGgTYprhydM/+GwRUzsm2JgIIaMKVv7lWgGd+W3kYTlEroHVU9lcSQEAlqFHbEDCKbOW5VQIMXzorxztuc5ulSsBxDK5kEUBAFsngOQgIgsu6DQFZJIPnICAILus2BGSRDJ6DgCC4rNsQkEUyeA4CguCybkNAFsngOVUC7vjGhzMebrNUCTgbhuv9BBDQz7DrBAR04eu/uVaA20tnbHLDKWoFxAbmrksCCLhENHZDgwDD/Dax8ey/QUDTtGyuJICASlCjtrUJ8ExxDRvbztsErKPaTvJGg3PPzQLezMmPAgQiApwfqCMC824jAtYhzacqGvz7DAooE7LoJBAX4P9w+Xe4yosLWG92ntC5t31ougTYOrgL/d4EbCbdpnXrZ/+8H9e9CXBzcC/6K70cAetBDpM79HB8xt9fyfzYyq2S5B0Qd0S/4UpLQPE8n8X8imXY/kV+AkpPE6Jwa/QbqPwEFAGj6Yw+vwwydDEwAfu+E9PwGdwLnEkC/te7+rclZefT4sO4l+lmCyiF18VlLD4V+g8Iv2HI/cBu64G/hN1G9ewHAWIvCECAmIC4PAlAgJiAuDwJQICYgLg8CUCAmIC4PAlAgJiAuDwJQICYgLg8CRAL+ALMTp/Tr/ULFAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "l = []\n",
    "for i in range(5000):\n",
    "    img = Image.new(\"RGB\",(128,128),\"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Stay in side\n",
    "    radius = random.randint(10,40)\n",
    "    # For now we ensure the shape is inside the canva\n",
    "    center = (random.randint(radius,128-radius),random.randint(radius,128-radius))\n",
    "    draw.circle(xy=center,radius=radius,fill=\"black\")\n",
    "    l.append(img)\n",
    "    img.save(f\"data/train/circles/{i}.png\")\n",
    "\n",
    "[display(img) for img in l[50:52]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbde7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(5000):\n",
    "    img = Image.new(\"RGB\",(128,128),\"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    points = [\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "]\n",
    "    draw.polygon(points,fill=\"black\")\n",
    "    l.append(img)\n",
    "    img.save(f\"data/train/triangles/{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfa279",
   "metadata": {},
   "source": [
    "# Third make the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2384bb",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c92535bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4a31fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapeCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=16384, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A nice way to get the accelerato (whonever is training)\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class ShapeCNN(nn.Module):\n",
    "    # Define the layers of the neural net\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride = 1,padding=1)\n",
    "\n",
    "\n",
    "        # Diminish our image size by 2, note that does make it so it need to be divisible by 2 our input\n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        #  Convert our bias and weight that we got from the neurons into a 256 a number utilized mostly due to convenicnce\n",
    "        # So we dont have overfitting, this applies an afiine linear transformation which is in summary\n",
    "        # A fancy way transformation of our previous vector into another vector, which follows a series of guidelines\n",
    "        self.fc1 = nn.Linear(in_features=(64*16*16),out_features=256)\n",
    "        self.fc2 = nn.Linear(256,2)\n",
    "\n",
    "\n",
    "\n",
    "    # How tensors are treated as we move along the net neurons\n",
    "    def forward(self,x):\n",
    "        # Here we are in summary ignoring negative values and making positive values variant\n",
    "        # A good metaphor is thinking that we are only getting the good tastes and not the bad ones\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "\n",
    "        # This immediatelly creates the layer and inputs the x given by our neurons layers\n",
    "        x = nn.Flatten()(x)\n",
    "\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x \n",
    "\n",
    "    \n",
    "\n",
    "# A nice way to see my model layers\n",
    "model = ShapeCNN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef429021",
   "metadata": {},
   "source": [
    "# Step 4 - Get my training data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a152b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ecdcd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data/train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.ImageFolder(\"data/train\",transform=transform)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8ae438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ebf10d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 128, 128])\n",
      "Labels batch shape: torch.Size([64])\n",
      "[1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# Notice how we have inherited labels according to our previous folder division\n",
    "print(train_labels.tolist()[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d60acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "def train_loop(model, train_loader,loss_fn,optimizer):\n",
    "    size = len(train_loader.dataset)\n",
    "    # You need to tell the model he is in training mode\n",
    "    model.train()\n",
    "    for batch,(images,labels) in enumerate(train_loader):\n",
    "        # Mode the data into the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Attempt at predict - Learning part\n",
    "        pred = model(images)\n",
    "        loss = loss_fn(pred,labels)\n",
    "        # Backpropagate - THE CHAIN RULE\n",
    "        loss.backward()\n",
    "        # Applies the gradients found by backpropagation\n",
    "        optimizer.step()\n",
    "        # Reset to not resum\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss = loss.item()\n",
    "            at = batch* batch_size\n",
    "            print(f\"Your current loss {loss:>7f} at {at} size {size}\")            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "903d0912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current loss 0.690957 at 0 size 10000\n",
      "Your current loss 0.685845 at 640 size 10000\n",
      "Your current loss 0.667386 at 1280 size 10000\n",
      "Your current loss 0.509525 at 1920 size 10000\n",
      "Your current loss 0.451500 at 2560 size 10000\n",
      "Your current loss 0.513071 at 3200 size 10000\n",
      "Your current loss 0.437530 at 3840 size 10000\n",
      "Your current loss 0.369108 at 4480 size 10000\n",
      "Your current loss 0.351385 at 5120 size 10000\n",
      "Your current loss 0.340055 at 5760 size 10000\n",
      "Your current loss 0.297571 at 6400 size 10000\n",
      "Your current loss 0.274247 at 7040 size 10000\n",
      "Your current loss 0.407083 at 7680 size 10000\n",
      "Your current loss 0.306669 at 8320 size 10000\n",
      "Your current loss 0.187594 at 8960 size 10000\n",
      "Your current loss 0.117811 at 9600 size 10000\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "train_loop(model=model,train_loader=train_loader,loss_fn = loss_fn,optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b792a",
   "metadata": {},
   "source": [
    "# Step 5 - Testing part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92fd641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(500):\n",
    "    img = Image.new(\"RGB\",(128,128),\"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    points = [\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "    (random.randint(10, 118), random.randint(10, 118)),\n",
    "]\n",
    "    draw.polygon(points,fill=\"black\")\n",
    "    l.append(img)\n",
    "    img.save(f\"data/test/triangles/{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84e825a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    img = Image.new(\"RGB\",(128,128),\"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Stay in side\n",
    "    radius = random.randint(10,40)\n",
    "    # For now we ensure the shape is inside the canva\n",
    "    center = (random.randint(radius,128-radius),random.randint(radius,128-radius))\n",
    "    draw.circle(xy=center,radius=radius,fill=\"black\")\n",
    "    l.append(img)\n",
    "    img.save(f\"data/test/circles/{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e27a87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "test_dataset = datasets.ImageFolder(\"data/test\",transform=transform)\n",
    "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d1f901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(test_loader,model,loss_fn):\n",
    "    model.eval()\n",
    "    size = len(test_loader.dataset)\n",
    "    num_batches = len(test_loader)\n",
    "    # Ensures we dont change the gradients while testing\n",
    "    with torch.no_grad():\n",
    "        test_loss, correct = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(images)\n",
    "            test_loss += loss_fn(pred, labels).item()\n",
    "            correct += (pred.argmax(1) == labels).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eeeeca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.005040 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader=test_loader,model=model,loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "037a4971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Your current loss 0.000038 at 0 size 10000\n",
      "Your current loss 0.000074 at 640 size 10000\n",
      "Your current loss 0.000025 at 1280 size 10000\n",
      "Your current loss 0.000050 at 1920 size 10000\n",
      "Your current loss 0.000081 at 2560 size 10000\n",
      "Your current loss 0.000028 at 3200 size 10000\n",
      "Your current loss 0.000240 at 3840 size 10000\n",
      "Your current loss 0.000042 at 4480 size 10000\n",
      "Your current loss 0.000017 at 5120 size 10000\n",
      "Your current loss 0.000030 at 5760 size 10000\n",
      "Your current loss 0.000053 at 6400 size 10000\n",
      "Your current loss 0.000051 at 7040 size 10000\n",
      "Your current loss 0.000047 at 7680 size 10000\n",
      "Your current loss 0.000008 at 8320 size 10000\n",
      "Your current loss 0.000025 at 8960 size 10000\n",
      "Your current loss 0.000055 at 9600 size 10000\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.004738 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Your current loss 0.000039 at 0 size 10000\n",
      "Your current loss 0.000099 at 640 size 10000\n",
      "Your current loss 0.000216 at 1280 size 10000\n",
      "Your current loss 0.000079 at 1920 size 10000\n",
      "Your current loss 0.000052 at 2560 size 10000\n",
      "Your current loss 0.000051 at 3200 size 10000\n",
      "Your current loss 0.000044 at 3840 size 10000\n",
      "Your current loss 0.000027 at 4480 size 10000\n",
      "Your current loss 0.000043 at 5120 size 10000\n",
      "Your current loss 0.000020 at 5760 size 10000\n",
      "Your current loss 0.000011 at 6400 size 10000\n",
      "Your current loss 0.000068 at 7040 size 10000\n",
      "Your current loss 0.000032 at 7680 size 10000\n",
      "Your current loss 0.000009 at 8320 size 10000\n",
      "Your current loss 0.000038 at 8960 size 10000\n",
      "Your current loss 0.000030 at 9600 size 10000\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.004931 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Your current loss 0.000016 at 0 size 10000\n",
      "Your current loss 0.000004 at 640 size 10000\n",
      "Your current loss 0.000025 at 1280 size 10000\n",
      "Your current loss 0.000020 at 1920 size 10000\n",
      "Your current loss 0.000045 at 2560 size 10000\n",
      "Your current loss 0.000046 at 3200 size 10000\n",
      "Your current loss 0.000023 at 3840 size 10000\n",
      "Your current loss 0.000019 at 4480 size 10000\n",
      "Your current loss 0.000009 at 5120 size 10000\n",
      "Your current loss 0.000012 at 5760 size 10000\n",
      "Your current loss 0.000018 at 6400 size 10000\n",
      "Your current loss 0.000017 at 7040 size 10000\n",
      "Your current loss 0.000039 at 7680 size 10000\n",
      "Your current loss 0.000046 at 8320 size 10000\n",
      "Your current loss 0.000031 at 8960 size 10000\n",
      "Your current loss 0.000011 at 9600 size 10000\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.004878 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Your current loss 0.000044 at 0 size 10000\n",
      "Your current loss 0.000016 at 640 size 10000\n",
      "Your current loss 0.000006 at 1280 size 10000\n",
      "Your current loss 0.000015 at 1920 size 10000\n",
      "Your current loss 0.000004 at 2560 size 10000\n",
      "Your current loss 0.000007 at 3200 size 10000\n",
      "Your current loss 0.000014 at 3840 size 10000\n",
      "Your current loss 0.000025 at 4480 size 10000\n",
      "Your current loss 0.000009 at 5120 size 10000\n",
      "Your current loss 0.000119 at 5760 size 10000\n",
      "Your current loss 0.000014 at 6400 size 10000\n",
      "Your current loss 0.000008 at 7040 size 10000\n",
      "Your current loss 0.000046 at 7680 size 10000\n",
      "Your current loss 0.000021 at 8320 size 10000\n",
      "Your current loss 0.000048 at 8960 size 10000\n",
      "Your current loss 0.000035 at 9600 size 10000\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.004594 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Your current loss 0.000015 at 0 size 10000\n",
      "Your current loss 0.000018 at 640 size 10000\n",
      "Your current loss 0.000016 at 1280 size 10000\n",
      "Your current loss 0.000019 at 1920 size 10000\n",
      "Your current loss 0.000005 at 2560 size 10000\n",
      "Your current loss 0.000007 at 3200 size 10000\n",
      "Your current loss 0.000021 at 3840 size 10000\n",
      "Your current loss 0.000005 at 4480 size 10000\n",
      "Your current loss 0.000002 at 5120 size 10000\n",
      "Your current loss 0.000013 at 5760 size 10000\n",
      "Your current loss 0.000010 at 6400 size 10000\n",
      "Your current loss 0.000021 at 7040 size 10000\n",
      "Your current loss 0.000016 at 7680 size 10000\n",
      "Your current loss 0.000007 at 8320 size 10000\n",
      "Your current loss 0.000002 at 8960 size 10000\n",
      "Your current loss 0.000010 at 9600 size 10000\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.004593 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model=model,train_loader=train_loader,loss_fn = loss_fn,optimizer = optimizer)\n",
    "    eval_loop(test_loader=test_loader,model=model,loss_fn=loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a03fdb",
   "metadata": {},
   "source": [
    "# Okay i am relatively happy with what we got lets throw this in to python as I am having a hard time tracking stuff, also lets try pytorch ignite to select the best moment of our epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00971cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
